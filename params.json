{"name":"Wellsfargoanalytics","tagline":"Analysis of social media content","body":"### Background\r\nWells Fargo provided us with a dataset containing social media posts from Twitter and Facebook. Our team was tasked with cleaning, and analyzing the data, then providing business insights based on the results. My teammates Alex and Kaya were the coders, while Zach and I analyzed and delivered the insights. With our team divided into two roles, communication between the coders and analysts were vital so the data they were mining for was coordinated with what business aspects we wanted to examine.\r\n![](<a href=\"http://imgur.com/1JS6hAq\"><img src=\"http://i.imgur.com/1JS6hAq.png\" title=\"source: imgur.com\" /></a>)\r\n\r\n### Methodology\r\nWe took the dataset which was given to us, then created a smaller data frame and analyze those posts making inferences about the larger population. From this smaller data frame we cleaned the data to extract the nonsensical words which would be of zero value to us in our analysis. All this code is under “Part A” under the Code portion.\r\n####Uncleaned data:\r\n![](http://i.imgur.com/hDTQPM9.png)\r\n\r\n####Cleaned data:\r\n![](http://i.imgur.com/vSTViUx.png)\r\nFrom this, we used lists of positive and negative words and matched those words to text in the social media posts. A sentiment score for each post was calculated by the number of positive words minus the negative words. Very positive and very negative posts were defined as the score being >= 2 or <= 2 respectively. This data frame was further divided this into data frames for posts mentioning each bank respectively. Also, we made a data frame for posts containing \"name\". Finally, we ran the sentiment analysis on these subsets and calculated their scores and made graphs.\r\nAdditionally, from the sample database, we created 10 word clouds, one \"very positive\" and one \"very negative\" for each bank and then the same thing for all four banks combined, demonstrating what conversations were taking in regards to the banks. \r\n\r\nPOSTS → 1000 Post DF → Smaller DFs of Posts → Sentiment Analysis\r\n\r\n![](http://i.imgur.com/GD93wxf.png)\r\n\r\n### Data\r\n Part A: Cleaning of the Data\r\n``` R df = read.table('dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n\r\n##### Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n#####test 10,000 texts: good sample size\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n##### Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n#####REMEMBER: IT MATTERS THE ORDER IN WHICH TM_MAP EXPRESSIONS ARE RUN\r\nlibrary(rJava)\r\ndocs <- tm_map(docs, content_transformer(tolower)) # convert to lowercase first\r\ndocs <- tm_map(docs, removeWords, stopwords('english'))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind = \"SMART\"))\r\n\r\n#####metaData/recurrent words\r\n\r\nmyMeta <- c(\"name\",\"bank\",\"banka\",\"bankb\",\"bankc\", \"bankd\", \"banke\",\r\n            \"internet\",\"https\", \"rettwit\", \"twithndl\", \"twit_hndl_banka\",\r\n            \"twit_hndl_bankb\",\"twit_hndl_bankc\",\"twit_hndl_bankd\", \"phone\",\r\n            \"dirmsg\", \"street\",\"name_resp\",\"bankds\", \"and\", \"for\", \"the\", \"you\",\r\n            \"twithndlbanka\",\"dir_msg\",\"ret_twit\",\"twit_hndl\")\r\ndocs <- tm_map(docs, removeWords, myMeta)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\ndtm <- DocumentTermMatrix(bankA.docs)\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n\r\n#####creates new data frame out of cleaned/processed text\r\nnew.df <-data.frame(text=unlist(sapply(dtm, `[`, \"content\")), stringsAsFactors=F)\r\n#####THIS ENDS PREPROCESSING\r\n\r\n\r\n\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\ndf$BankID = vector(mode=\"numeric\",length = nrow(df))\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx] \r\n\r\n\r\ndtm <- DocumentTermMatrix(bankA.docs)\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n\r\n######creates new data frame out of cleaned/processed text\r\nnew.df <-data.frame(text=unlist(sapply(dtm, `[`, \"content\")), stringsAsFactors=F) #Posts that have been cleaned\r\n\r\nfindFreqTerms(dtm,300)\r\n\r\nfreq <- colSums(as.matrix(dtm))  \r\nfreq\r\nord <- order(freq)   \r\n\r\nlibrary(wordcloud)\r\nwordcloud(names(freq), freq, colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\nSentiment Analysis Code\r\n\r\n# Since we can't find a great package in R, I'm going to use an\r\n# example I found online to build our own\r\n# Based on: http://www.ihub.co.ke/blogs/23216\r\n\r\n# Only need to do once\r\n# Download and upload: http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar\r\n#system('unrar e opinion-lexicon-English.rar')\r\n\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  # we got a vector of sentences. plyr will handle a list\r\n  # or a vector as an \"l\" for us\r\n  # we want a simple array (\"a\") of scores back, so we use \r\n  # \"l\" + \"a\" + \"ply\" = \"laply\":\r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    # and convert to lower case:\r\n    sentence = tolower(sentence)\r\n    \r\n    # split into words. str_split is in the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # compare our words to the dictionaries of positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    # we just want a TRUE/FALSE:\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\n#breaks data frame into manageable size\r\ndf.1000$FullText = as.character(df.1000$FullText)\r\n\r\n#subsets data frame into posts that mention Bank A, “name”, etc\r\ndf.bankA = df.1000[bankA.idx,]\r\ndf.bankB = df.1000[bankB.idx,]\r\ndf.bankC = df.1000[bankC.idx,]\r\ndf.bankD = df.1000[bankD.idx,]\r\ndf.names = df.1000[name.idx,]\r\ndf.internet = df.1000[internet.idx,]\r\n\r\n#sets the data frame whose scores we will calculate\r\ndf.sent = df.names\r\n\r\nscores = score.sentiment(df.sent$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\n\r\nscores$mediatype = df.sent$MediaType\r\n\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\n  geom_boxplot(aes(fill=mediatype)) +\r\n  scale_fill_manual(values=cols) +\r\n  geom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\n  labs(title = \"Media Type's Sentiment Scores: Text including 'name'\") + \r\n  xlab('Media Type') + ylab('Sentiment Score')\r\n\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\n\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Sentiment Score: Text including 'name'\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\n\r\nggplot(mediatype_pos, aes(x = factor(mediatype), y = mean_pos, fill=mediatype)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Positive Sentiment Score: Text including 'name'\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_neg$mediatype, mediatype_neg$mean_neg)\r\n\r\nggplot(mediatype_neg, aes(x = factor(mediatype), y = mean_neg, fill=mediatype)) +\r\n  geom_bar(stat=\"identity\") +\r\n  scale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\n  labs(title = \"Average Very Negative Sentiment Score: Text including 'name'\") + \r\n  xlab('Media Type') + ylab('Average Score')\r\n\r\n==========================================================\r\nNEW\r\n\r\nnew.df.bankA <-data.frame(text=unlist(sapply(bankA.docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\nnew.df.bankB <-data.frame(text=unlist(sapply(bankB.docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\nnew.df.bankC <-data.frame(text=unlist(sapply(bankC.docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\nnew.df.bankD <-data.frame(text=unlist(sapply(bankD.docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\nnew.df.name <-data.frame(text=unlist(sapply(name.docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\nnew.df.internet <-data.frame(text=unlist(sapply(internet.docs, `[[`, \"content\")), stringsAsFactors=FALSE)\r\n\r\n\r\n\r\ndf.sent = new.df.bankA\r\n\r\nscores = score.sentiment(df.sent$text, pos, neg, .progress='text')\r\n\r\nlibrary(tm)\r\nlibrary(wordcloud)\r\n\r\nscores$text = as.character(scores$text)\r\n\r\nposIndices = which(as.numeric(scores$score) > 1)\r\nnegIndices = which(as.numeric(scores$score) < -1)\r\n\r\nposPosts = scores[posIndices,2]\r\nnegPosts = scores[negIndices,2]\r\n\r\nwordcloud(posPosts, min.freq = 5, colors=brewer.pal(8, \"Dark2\"))\r\n\r\n\r\nGlobal sentiment scores  (global_score = round( 100 * numpos / (numpos + numneg) )\r\n\r\nAll Banks (df.1000): 59\r\nBank A: 57\r\nBank B: 56\r\nBank C: 74\r\nBank D: 50\r\nText with “NAME”: 50\r\nText with “INTERNET”: 55 ->\r\n```\r\n\r\n### Analysis\r\nThe word clusters below are a combination of all the words about a given bank. The next group of clusters are grouped into positive and negative clusters so we have more context to analyze them in and can draw more conclusions.\r\n#### Bank A\r\n![](http://i.imgur.com/LFu549B.png)\r\n#### Bank B\r\n![](http://i.imgur.com/D9Z00Ay.png)\r\n#### Bank C\r\n![](http://i.imgur.com/Z2HFK5N.png)\r\n#### Bank D\r\nhttp://i.imgur.com/xc9FNhD.png\r\n#### All Banks\r\n![](http://i.imgur.com/ULvKtEk.png)\r\n\r\nAll the word clusters above are created from the sample data frame we created and for a word to appear, it had to have been used at least 50 times. As one would assume, each bank had topics that were specific to it, but there were some overarching themes for all four banks. The most prevalent words were “call” and phone” which appeared in all the banks’ word cluster with the exception being Bank D. The appearance of “call” and “phone” can be linked to the importance of customer service, especially with the banks’ call centers. With the increasing number of people utilizing the remote call centers, it is increasingly important for those people in the centers to be properly trained and held to the highest standard for their customer service. Banks should consider expanding their budgets for training those individuals to ensure the best customer experience and retain current clientele.\r\nAs for the financial topics being discussed via social media, Bank D’s data was the most helpful providing areas such as “financial management”, “grants”, “advisers” while the remaining banks had more broad terms such as “credit card”. This may due to a policy Bank D has where they encourage their customers to leave specific comments so when doing analysis like this, it is easier to have more solid takeaways.\r\n\r\n\r\nThe bank that stood out as being different in the word clusters was Bank D. While all other banks shared words “like” and “can,” Bank D showed more active words such as “swing,” “apply,” and “program.” Bank D may be more effective on a basic customer service level, therefore prompting tweets only when more complex issues come about.\r\n\r\n####Positive and Negative Word Clusters\r\nBank A\r\n![](http://i.imgur.com/5yxwLGH.png)\r\nThere are very few constructive positive takeaways we can have with Bank A’s positive sentiment word cluster. Although there are are words such as “personal”, “service”, “nice”, and “free” which all suggest a positive experience, the negative sentiment word cluster seems to carry more weight with words such as “customer”, “service”, “fraud”, “fucking”, “time”, and “dont”. Other negative words include “worst”, “scam”, and “lost” all which lead me to believe that the customers at Bank A believe their bank to be incompetent and have trust issues.\r\n\r\nBank B\r\n![](http://i.imgur.com/d5Gp5hv.png)\r\nBank B is very similar to Bank A in the respect that there seems to be trust issues, especially relating to security, with the customers and the bank. In the negative word cloud there are words like “card”, “scam”, “settlement”, “lose” which suggests that there is a lapse in the protocol relating to security for Bank B and possibly their banking cards. On the positive side, customer service seems to be a strong suit for Bank B as one of their most prominent positive words is “support”, “time”, and “team” leading me to believe that Bank B works very efficiently and succinctly in providing the necessary customer service.\r\n\r\nBank C\r\n![](http://i.imgur.com/HSNiDW0.png)\r\nIt appears that Bank C’s customers really enjoy whatever “free” services they offer, as well as overwhelming “support”. These positive reports are countered by people running into issues with their “card”, “account”, and “fraud”. Bank C’s problem lies not with their average worker, but with broader institutional framework that allows their customers’ accounts to be compromised.\r\n\r\nBank D\r\n![](http://i.imgur.com/rfUCPCl.png)\r\nBank D appears to be having some fundamental issues with aspects of banking like “money”, “card”, and “fraud”, but their customer service doesn’t seem to be an issue.\r\n\r\nAll Banks\r\nPositive Word Cloud\r\n![](http://i.imgur.com/Q26IkaB.png)\r\nNegative Word Cloud\r\n![](http://i.imgur.com/8V93D77.png)\r\nWith words relating to time appearing in the positive word cloud like “day” and “today”, one can infer that all the banks are very timely in the services they provide, whether they are customer services or financial. However, looking at the negative word cloud, customers do not seem pleased with the quality of service provided with words such as “money”, “customer”, “service”, “account”, and “card” appearing.\r\n\r\n####Sentiment Analysis\r\nThe following table displays the sentiment analysis scores for all the banks on both Twitter and Facebook. In all the sentiment scores except for Bank B and C's average, Facebook and Twitter's signs were always the same indicating consistent reviews on the platforms but to varying degrees. If you look at All Banks' score for each sentiment score, Facebook's is nearly double that of Twitter demonstrating that people are more opinionated on Facebook than Twitter which makes sense because there is a character limit on Twitter so people can not go on rants like they could on Facebook. Although this is might be extreme, what companies can do to deal with this is to take the time to respond to the customers' posts either reinforcing the positive posts or attempting to amend the customer's issues.\r\n![](http://i.imgur.com/IfCf1q5.png)\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}